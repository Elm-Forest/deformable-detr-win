{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Try it in Google Colab"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git clone https://github.com/fundamentalvision/Deformable-DETR.git"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "id": "jbuc4mAlIxm9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%cd /content/Deformable-DETR"
   ],
   "metadata": {
    "id": "0m2YOnWOJnx9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%cd ./models/ops"
   ],
   "metadata": {
    "id": "Ext6Ij8lJVUc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!sh ./make.sh"
   ],
   "metadata": {
    "id": "lqt9G7AAJeXw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# unit test (should see all checking is True)\n",
    "!python test.py"
   ],
   "metadata": {
    "id": "omU4jr5DJfh3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%cd /content/Deformable-DETR"
   ],
   "metadata": {
    "id": "rEPhJFyndvvt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!wget http://images.cocodataset.org/zips/train2017.zip"
   ],
   "metadata": {
    "id": "zdCf3qb2OMNZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!wget http://images.cocodataset.org/zips/test2017.zip"
   ],
   "metadata": {
    "id": "ASv2wK9_OcGU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!wget http://images.cocodataset.org/zips/val2017.zip"
   ],
   "metadata": {
    "id": "bcrPmqlxOfXr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip"
   ],
   "metadata": {
    "id": "8d68rSxiPWSl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!unzip -o -d /content/Deformable-DETR/coco ./train2017.zip"
   ],
   "metadata": {
    "id": "DEc2o4DQOoMP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!rm -f train2017.zip"
   ],
   "metadata": {
    "id": "s1CVo_KwP3Rr"
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!unzip -o -d /content/Deformable-DETR/coco ./test2017.zip"
   ],
   "metadata": {
    "id": "-XJRFLdfPGOU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!unzip -o -d /content/Deformable-DETR/coco ./val2017.zip"
   ],
   "metadata": {
    "id": "syEfu1RzPIdu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!unzip -o -d /content/Deformable-DETR/coco ./annotations_trainval2017.zip"
   ],
   "metadata": {
    "id": "o656yiS8cTtd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!python main.py --epochs 50 --coco_path /content/Deformable-DETR/coco"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sTXZygzwcaqE",
    "outputId": "2f9a75bc-7b8f-41ab-e5cc-b4629bec9691"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Not using distributed mode\n",
      "git:\n",
      "  sha: 11169a60c33333af00a4849f1808023eba96a931, status: has uncommited changes, branch: main\n",
      "\n",
      "Namespace(aux_loss=True, backbone='resnet50', batch_size=2, bbox_loss_coef=5, cache_mode=False, clip_max_norm=0.1, cls_loss_coef=2, coco_panoptic_path=None, coco_path='/content/Deformable-DETR/coco', dataset_file='coco', dec_layers=6, dec_n_points=4, device='cuda', dice_loss_coef=1, dilation=False, dim_feedforward=1024, distributed=False, dropout=0.1, enc_layers=6, enc_n_points=4, epochs=50, eval=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2, hidden_dim=256, lr=0.0002, lr_backbone=2e-05, lr_backbone_names=['backbone.0'], lr_drop=40, lr_drop_epochs=None, lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], mask_loss_coef=1, masks=False, nheads=8, num_feature_levels=4, num_queries=300, num_workers=2, output_dir='', position_embedding='sine', position_embedding_scale=6.283185307179586, remove_difficult=False, resume='', seed=42, set_cost_bbox=5, set_cost_class=2, set_cost_giou=2, sgd=False, start_epoch=0, two_stage=False, weight_decay=0.0001, with_box_refine=False)\n",
      "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
      "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100% 97.8M/97.8M [00:00<00:00, 277MB/s]\n",
      "number of params: 39847265\n",
      "loading annotations into memory...\n",
      "Done (t=15.40s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=2.12s)\n",
      "creating index...\n",
      "index created!\n",
      "transformer.level_embed\n",
      "transformer.encoder.layers.0.self_attn.sampling_offsets.weight\n",
      "transformer.encoder.layers.0.self_attn.sampling_offsets.bias\n",
      "transformer.encoder.layers.0.self_attn.attention_weights.weight\n",
      "transformer.encoder.layers.0.self_attn.attention_weights.bias\n",
      "transformer.encoder.layers.0.self_attn.value_proj.weight\n",
      "transformer.encoder.layers.0.self_attn.value_proj.bias\n",
      "transformer.encoder.layers.0.self_attn.output_proj.weight\n",
      "transformer.encoder.layers.0.self_attn.output_proj.bias\n",
      "transformer.encoder.layers.0.norm1.weight\n",
      "transformer.encoder.layers.0.norm1.bias\n",
      "transformer.encoder.layers.0.linear1.weight\n",
      "transformer.encoder.layers.0.linear1.bias\n",
      "transformer.encoder.layers.0.linear2.weight\n",
      "transformer.encoder.layers.0.linear2.bias\n",
      "transformer.encoder.layers.0.norm2.weight\n",
      "transformer.encoder.layers.0.norm2.bias\n",
      "transformer.encoder.layers.1.self_attn.sampling_offsets.weight\n",
      "transformer.encoder.layers.1.self_attn.sampling_offsets.bias\n",
      "transformer.encoder.layers.1.self_attn.attention_weights.weight\n",
      "transformer.encoder.layers.1.self_attn.attention_weights.bias\n",
      "transformer.encoder.layers.1.self_attn.value_proj.weight\n",
      "transformer.encoder.layers.1.self_attn.value_proj.bias\n",
      "transformer.encoder.layers.1.self_attn.output_proj.weight\n",
      "transformer.encoder.layers.1.self_attn.output_proj.bias\n",
      "transformer.encoder.layers.1.norm1.weight\n",
      "transformer.encoder.layers.1.norm1.bias\n",
      "transformer.encoder.layers.1.linear1.weight\n",
      "transformer.encoder.layers.1.linear1.bias\n",
      "transformer.encoder.layers.1.linear2.weight\n",
      "transformer.encoder.layers.1.linear2.bias\n",
      "transformer.encoder.layers.1.norm2.weight\n",
      "transformer.encoder.layers.1.norm2.bias\n",
      "transformer.encoder.layers.2.self_attn.sampling_offsets.weight\n",
      "transformer.encoder.layers.2.self_attn.sampling_offsets.bias\n",
      "transformer.encoder.layers.2.self_attn.attention_weights.weight\n",
      "transformer.encoder.layers.2.self_attn.attention_weights.bias\n",
      "transformer.encoder.layers.2.self_attn.value_proj.weight\n",
      "transformer.encoder.layers.2.self_attn.value_proj.bias\n",
      "transformer.encoder.layers.2.self_attn.output_proj.weight\n",
      "transformer.encoder.layers.2.self_attn.output_proj.bias\n",
      "transformer.encoder.layers.2.norm1.weight\n",
      "transformer.encoder.layers.2.norm1.bias\n",
      "transformer.encoder.layers.2.linear1.weight\n",
      "transformer.encoder.layers.2.linear1.bias\n",
      "transformer.encoder.layers.2.linear2.weight\n",
      "transformer.encoder.layers.2.linear2.bias\n",
      "transformer.encoder.layers.2.norm2.weight\n",
      "transformer.encoder.layers.2.norm2.bias\n",
      "transformer.encoder.layers.3.self_attn.sampling_offsets.weight\n",
      "transformer.encoder.layers.3.self_attn.sampling_offsets.bias\n",
      "transformer.encoder.layers.3.self_attn.attention_weights.weight\n",
      "transformer.encoder.layers.3.self_attn.attention_weights.bias\n",
      "transformer.encoder.layers.3.self_attn.value_proj.weight\n",
      "transformer.encoder.layers.3.self_attn.value_proj.bias\n",
      "transformer.encoder.layers.3.self_attn.output_proj.weight\n",
      "transformer.encoder.layers.3.self_attn.output_proj.bias\n",
      "transformer.encoder.layers.3.norm1.weight\n",
      "transformer.encoder.layers.3.norm1.bias\n",
      "transformer.encoder.layers.3.linear1.weight\n",
      "transformer.encoder.layers.3.linear1.bias\n",
      "transformer.encoder.layers.3.linear2.weight\n",
      "transformer.encoder.layers.3.linear2.bias\n",
      "transformer.encoder.layers.3.norm2.weight\n",
      "transformer.encoder.layers.3.norm2.bias\n",
      "transformer.encoder.layers.4.self_attn.sampling_offsets.weight\n",
      "transformer.encoder.layers.4.self_attn.sampling_offsets.bias\n",
      "transformer.encoder.layers.4.self_attn.attention_weights.weight\n",
      "transformer.encoder.layers.4.self_attn.attention_weights.bias\n",
      "transformer.encoder.layers.4.self_attn.value_proj.weight\n",
      "transformer.encoder.layers.4.self_attn.value_proj.bias\n",
      "transformer.encoder.layers.4.self_attn.output_proj.weight\n",
      "transformer.encoder.layers.4.self_attn.output_proj.bias\n",
      "transformer.encoder.layers.4.norm1.weight\n",
      "transformer.encoder.layers.4.norm1.bias\n",
      "transformer.encoder.layers.4.linear1.weight\n",
      "transformer.encoder.layers.4.linear1.bias\n",
      "transformer.encoder.layers.4.linear2.weight\n",
      "transformer.encoder.layers.4.linear2.bias\n",
      "transformer.encoder.layers.4.norm2.weight\n",
      "transformer.encoder.layers.4.norm2.bias\n",
      "transformer.encoder.layers.5.self_attn.sampling_offsets.weight\n",
      "transformer.encoder.layers.5.self_attn.sampling_offsets.bias\n",
      "transformer.encoder.layers.5.self_attn.attention_weights.weight\n",
      "transformer.encoder.layers.5.self_attn.attention_weights.bias\n",
      "transformer.encoder.layers.5.self_attn.value_proj.weight\n",
      "transformer.encoder.layers.5.self_attn.value_proj.bias\n",
      "transformer.encoder.layers.5.self_attn.output_proj.weight\n",
      "transformer.encoder.layers.5.self_attn.output_proj.bias\n",
      "transformer.encoder.layers.5.norm1.weight\n",
      "transformer.encoder.layers.5.norm1.bias\n",
      "transformer.encoder.layers.5.linear1.weight\n",
      "transformer.encoder.layers.5.linear1.bias\n",
      "transformer.encoder.layers.5.linear2.weight\n",
      "transformer.encoder.layers.5.linear2.bias\n",
      "transformer.encoder.layers.5.norm2.weight\n",
      "transformer.encoder.layers.5.norm2.bias\n",
      "transformer.decoder.layers.0.cross_attn.sampling_offsets.weight\n",
      "transformer.decoder.layers.0.cross_attn.sampling_offsets.bias\n",
      "transformer.decoder.layers.0.cross_attn.attention_weights.weight\n",
      "transformer.decoder.layers.0.cross_attn.attention_weights.bias\n",
      "transformer.decoder.layers.0.cross_attn.value_proj.weight\n",
      "transformer.decoder.layers.0.cross_attn.value_proj.bias\n",
      "transformer.decoder.layers.0.cross_attn.output_proj.weight\n",
      "transformer.decoder.layers.0.cross_attn.output_proj.bias\n",
      "transformer.decoder.layers.0.norm1.weight\n",
      "transformer.decoder.layers.0.norm1.bias\n",
      "transformer.decoder.layers.0.self_attn.in_proj_weight\n",
      "transformer.decoder.layers.0.self_attn.in_proj_bias\n",
      "transformer.decoder.layers.0.self_attn.out_proj.weight\n",
      "transformer.decoder.layers.0.self_attn.out_proj.bias\n",
      "transformer.decoder.layers.0.norm2.weight\n",
      "transformer.decoder.layers.0.norm2.bias\n",
      "transformer.decoder.layers.0.linear1.weight\n",
      "transformer.decoder.layers.0.linear1.bias\n",
      "transformer.decoder.layers.0.linear2.weight\n",
      "transformer.decoder.layers.0.linear2.bias\n",
      "transformer.decoder.layers.0.norm3.weight\n",
      "transformer.decoder.layers.0.norm3.bias\n",
      "transformer.decoder.layers.1.cross_attn.sampling_offsets.weight\n",
      "transformer.decoder.layers.1.cross_attn.sampling_offsets.bias\n",
      "transformer.decoder.layers.1.cross_attn.attention_weights.weight\n",
      "transformer.decoder.layers.1.cross_attn.attention_weights.bias\n",
      "transformer.decoder.layers.1.cross_attn.value_proj.weight\n",
      "transformer.decoder.layers.1.cross_attn.value_proj.bias\n",
      "transformer.decoder.layers.1.cross_attn.output_proj.weight\n",
      "transformer.decoder.layers.1.cross_attn.output_proj.bias\n",
      "transformer.decoder.layers.1.norm1.weight\n",
      "transformer.decoder.layers.1.norm1.bias\n",
      "transformer.decoder.layers.1.self_attn.in_proj_weight\n",
      "transformer.decoder.layers.1.self_attn.in_proj_bias\n",
      "transformer.decoder.layers.1.self_attn.out_proj.weight\n",
      "transformer.decoder.layers.1.self_attn.out_proj.bias\n",
      "transformer.decoder.layers.1.norm2.weight\n",
      "transformer.decoder.layers.1.norm2.bias\n",
      "transformer.decoder.layers.1.linear1.weight\n",
      "transformer.decoder.layers.1.linear1.bias\n",
      "transformer.decoder.layers.1.linear2.weight\n",
      "transformer.decoder.layers.1.linear2.bias\n",
      "transformer.decoder.layers.1.norm3.weight\n",
      "transformer.decoder.layers.1.norm3.bias\n",
      "transformer.decoder.layers.2.cross_attn.sampling_offsets.weight\n",
      "transformer.decoder.layers.2.cross_attn.sampling_offsets.bias\n",
      "transformer.decoder.layers.2.cross_attn.attention_weights.weight\n",
      "transformer.decoder.layers.2.cross_attn.attention_weights.bias\n",
      "transformer.decoder.layers.2.cross_attn.value_proj.weight\n",
      "transformer.decoder.layers.2.cross_attn.value_proj.bias\n",
      "transformer.decoder.layers.2.cross_attn.output_proj.weight\n",
      "transformer.decoder.layers.2.cross_attn.output_proj.bias\n",
      "transformer.decoder.layers.2.norm1.weight\n",
      "transformer.decoder.layers.2.norm1.bias\n",
      "transformer.decoder.layers.2.self_attn.in_proj_weight\n",
      "transformer.decoder.layers.2.self_attn.in_proj_bias\n",
      "transformer.decoder.layers.2.self_attn.out_proj.weight\n",
      "transformer.decoder.layers.2.self_attn.out_proj.bias\n",
      "transformer.decoder.layers.2.norm2.weight\n",
      "transformer.decoder.layers.2.norm2.bias\n",
      "transformer.decoder.layers.2.linear1.weight\n",
      "transformer.decoder.layers.2.linear1.bias\n",
      "transformer.decoder.layers.2.linear2.weight\n",
      "transformer.decoder.layers.2.linear2.bias\n",
      "transformer.decoder.layers.2.norm3.weight\n",
      "transformer.decoder.layers.2.norm3.bias\n",
      "transformer.decoder.layers.3.cross_attn.sampling_offsets.weight\n",
      "transformer.decoder.layers.3.cross_attn.sampling_offsets.bias\n",
      "transformer.decoder.layers.3.cross_attn.attention_weights.weight\n",
      "transformer.decoder.layers.3.cross_attn.attention_weights.bias\n",
      "transformer.decoder.layers.3.cross_attn.value_proj.weight\n",
      "transformer.decoder.layers.3.cross_attn.value_proj.bias\n",
      "transformer.decoder.layers.3.cross_attn.output_proj.weight\n",
      "transformer.decoder.layers.3.cross_attn.output_proj.bias\n",
      "transformer.decoder.layers.3.norm1.weight\n",
      "transformer.decoder.layers.3.norm1.bias\n",
      "transformer.decoder.layers.3.self_attn.in_proj_weight\n",
      "transformer.decoder.layers.3.self_attn.in_proj_bias\n",
      "transformer.decoder.layers.3.self_attn.out_proj.weight\n",
      "transformer.decoder.layers.3.self_attn.out_proj.bias\n",
      "transformer.decoder.layers.3.norm2.weight\n",
      "transformer.decoder.layers.3.norm2.bias\n",
      "transformer.decoder.layers.3.linear1.weight\n",
      "transformer.decoder.layers.3.linear1.bias\n",
      "transformer.decoder.layers.3.linear2.weight\n",
      "transformer.decoder.layers.3.linear2.bias\n",
      "transformer.decoder.layers.3.norm3.weight\n",
      "transformer.decoder.layers.3.norm3.bias\n",
      "transformer.decoder.layers.4.cross_attn.sampling_offsets.weight\n",
      "transformer.decoder.layers.4.cross_attn.sampling_offsets.bias\n",
      "transformer.decoder.layers.4.cross_attn.attention_weights.weight\n",
      "transformer.decoder.layers.4.cross_attn.attention_weights.bias\n",
      "transformer.decoder.layers.4.cross_attn.value_proj.weight\n",
      "transformer.decoder.layers.4.cross_attn.value_proj.bias\n",
      "transformer.decoder.layers.4.cross_attn.output_proj.weight\n",
      "transformer.decoder.layers.4.cross_attn.output_proj.bias\n",
      "transformer.decoder.layers.4.norm1.weight\n",
      "transformer.decoder.layers.4.norm1.bias\n",
      "transformer.decoder.layers.4.self_attn.in_proj_weight\n",
      "transformer.decoder.layers.4.self_attn.in_proj_bias\n",
      "transformer.decoder.layers.4.self_attn.out_proj.weight\n",
      "transformer.decoder.layers.4.self_attn.out_proj.bias\n",
      "transformer.decoder.layers.4.norm2.weight\n",
      "transformer.decoder.layers.4.norm2.bias\n",
      "transformer.decoder.layers.4.linear1.weight\n",
      "transformer.decoder.layers.4.linear1.bias\n",
      "transformer.decoder.layers.4.linear2.weight\n",
      "transformer.decoder.layers.4.linear2.bias\n",
      "transformer.decoder.layers.4.norm3.weight\n",
      "transformer.decoder.layers.4.norm3.bias\n",
      "transformer.decoder.layers.5.cross_attn.sampling_offsets.weight\n",
      "transformer.decoder.layers.5.cross_attn.sampling_offsets.bias\n",
      "transformer.decoder.layers.5.cross_attn.attention_weights.weight\n",
      "transformer.decoder.layers.5.cross_attn.attention_weights.bias\n",
      "transformer.decoder.layers.5.cross_attn.value_proj.weight\n",
      "transformer.decoder.layers.5.cross_attn.value_proj.bias\n",
      "transformer.decoder.layers.5.cross_attn.output_proj.weight\n",
      "transformer.decoder.layers.5.cross_attn.output_proj.bias\n",
      "transformer.decoder.layers.5.norm1.weight\n",
      "transformer.decoder.layers.5.norm1.bias\n",
      "transformer.decoder.layers.5.self_attn.in_proj_weight\n",
      "transformer.decoder.layers.5.self_attn.in_proj_bias\n",
      "transformer.decoder.layers.5.self_attn.out_proj.weight\n",
      "transformer.decoder.layers.5.self_attn.out_proj.bias\n",
      "transformer.decoder.layers.5.norm2.weight\n",
      "transformer.decoder.layers.5.norm2.bias\n",
      "transformer.decoder.layers.5.linear1.weight\n",
      "transformer.decoder.layers.5.linear1.bias\n",
      "transformer.decoder.layers.5.linear2.weight\n",
      "transformer.decoder.layers.5.linear2.bias\n",
      "transformer.decoder.layers.5.norm3.weight\n",
      "transformer.decoder.layers.5.norm3.bias\n",
      "transformer.reference_points.weight\n",
      "transformer.reference_points.bias\n",
      "class_embed.0.weight\n",
      "class_embed.0.bias\n",
      "bbox_embed.0.layers.0.weight\n",
      "bbox_embed.0.layers.0.bias\n",
      "bbox_embed.0.layers.1.weight\n",
      "bbox_embed.0.layers.1.bias\n",
      "bbox_embed.0.layers.2.weight\n",
      "bbox_embed.0.layers.2.bias\n",
      "query_embed.weight\n",
      "input_proj.0.0.weight\n",
      "input_proj.0.0.bias\n",
      "input_proj.0.1.weight\n",
      "input_proj.0.1.bias\n",
      "input_proj.1.0.weight\n",
      "input_proj.1.0.bias\n",
      "input_proj.1.1.weight\n",
      "input_proj.1.1.bias\n",
      "input_proj.2.0.weight\n",
      "input_proj.2.0.bias\n",
      "input_proj.2.1.weight\n",
      "input_proj.2.1.bias\n",
      "input_proj.3.0.weight\n",
      "input_proj.3.0.bias\n",
      "input_proj.3.1.weight\n",
      "input_proj.3.1.bias\n",
      "backbone.0.body.conv1.weight\n",
      "backbone.0.body.layer1.0.conv1.weight\n",
      "backbone.0.body.layer1.0.conv2.weight\n",
      "backbone.0.body.layer1.0.conv3.weight\n",
      "backbone.0.body.layer1.0.downsample.0.weight\n",
      "backbone.0.body.layer1.1.conv1.weight\n",
      "backbone.0.body.layer1.1.conv2.weight\n",
      "backbone.0.body.layer1.1.conv3.weight\n",
      "backbone.0.body.layer1.2.conv1.weight\n",
      "backbone.0.body.layer1.2.conv2.weight\n",
      "backbone.0.body.layer1.2.conv3.weight\n",
      "backbone.0.body.layer2.0.conv1.weight\n",
      "backbone.0.body.layer2.0.conv2.weight\n",
      "backbone.0.body.layer2.0.conv3.weight\n",
      "backbone.0.body.layer2.0.downsample.0.weight\n",
      "backbone.0.body.layer2.1.conv1.weight\n",
      "backbone.0.body.layer2.1.conv2.weight\n",
      "backbone.0.body.layer2.1.conv3.weight\n",
      "backbone.0.body.layer2.2.conv1.weight\n",
      "backbone.0.body.layer2.2.conv2.weight\n",
      "backbone.0.body.layer2.2.conv3.weight\n",
      "backbone.0.body.layer2.3.conv1.weight\n",
      "backbone.0.body.layer2.3.conv2.weight\n",
      "backbone.0.body.layer2.3.conv3.weight\n",
      "backbone.0.body.layer3.0.conv1.weight\n",
      "backbone.0.body.layer3.0.conv2.weight\n",
      "backbone.0.body.layer3.0.conv3.weight\n",
      "backbone.0.body.layer3.0.downsample.0.weight\n",
      "backbone.0.body.layer3.1.conv1.weight\n",
      "backbone.0.body.layer3.1.conv2.weight\n",
      "backbone.0.body.layer3.1.conv3.weight\n",
      "backbone.0.body.layer3.2.conv1.weight\n",
      "backbone.0.body.layer3.2.conv2.weight\n",
      "backbone.0.body.layer3.2.conv3.weight\n",
      "backbone.0.body.layer3.3.conv1.weight\n",
      "backbone.0.body.layer3.3.conv2.weight\n",
      "backbone.0.body.layer3.3.conv3.weight\n",
      "backbone.0.body.layer3.4.conv1.weight\n",
      "backbone.0.body.layer3.4.conv2.weight\n",
      "backbone.0.body.layer3.4.conv3.weight\n",
      "backbone.0.body.layer3.5.conv1.weight\n",
      "backbone.0.body.layer3.5.conv2.weight\n",
      "backbone.0.body.layer3.5.conv3.weight\n",
      "backbone.0.body.layer4.0.conv1.weight\n",
      "backbone.0.body.layer4.0.conv2.weight\n",
      "backbone.0.body.layer4.0.conv3.weight\n",
      "backbone.0.body.layer4.0.downsample.0.weight\n",
      "backbone.0.body.layer4.1.conv1.weight\n",
      "backbone.0.body.layer4.1.conv2.weight\n",
      "backbone.0.body.layer4.1.conv3.weight\n",
      "backbone.0.body.layer4.2.conv1.weight\n",
      "backbone.0.body.layer4.2.conv2.weight\n",
      "backbone.0.body.layer4.2.conv3.weight\n",
      "Start training\n",
      "/content/Deformable-DETR/models/position_encoding.py:49: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
      "/usr/local/lib/python3.7/dist-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Epoch: [0]  [    0/59143]  eta: 5 days, 13:22:41  lr: 0.000200  class_error: 100.00  grad_norm: 71.36  loss: 42.4749 (42.4749)  loss_ce: 2.4400 (2.4400)  loss_bbox: 3.1043 (3.1043)  loss_giou: 1.6534 (1.6534)  loss_ce_0: 2.0776 (2.0776)  loss_bbox_0: 3.1815 (3.1815)  loss_giou_0: 1.6534 (1.6534)  loss_ce_1: 2.1029 (2.1029)  loss_bbox_1: 3.1815 (3.1815)  loss_giou_1: 1.6534 (1.6534)  loss_ce_2: 2.3884 (2.3884)  loss_bbox_2: 3.0927 (3.0927)  loss_giou_2: 1.6534 (1.6534)  loss_ce_3: 2.3529 (2.3529)  loss_bbox_3: 3.1201 (3.1201)  loss_giou_3: 1.6534 (1.6534)  loss_ce_4: 2.4158 (2.4158)  loss_bbox_4: 3.0970 (3.0970)  loss_giou_4: 1.6534 (1.6534)  loss_ce_unscaled: 1.2200 (1.2200)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6209 (0.6209)  loss_giou_unscaled: 0.8267 (0.8267)  cardinality_error_unscaled: 298.0000 (298.0000)  loss_ce_0_unscaled: 1.0388 (1.0388)  loss_bbox_0_unscaled: 0.6363 (0.6363)  loss_giou_0_unscaled: 0.8267 (0.8267)  cardinality_error_0_unscaled: 292.5000 (292.5000)  loss_ce_1_unscaled: 1.0514 (1.0514)  loss_bbox_1_unscaled: 0.6363 (0.6363)  loss_giou_1_unscaled: 0.8267 (0.8267)  cardinality_error_1_unscaled: 298.0000 (298.0000)  loss_ce_2_unscaled: 1.1942 (1.1942)  loss_bbox_2_unscaled: 0.6185 (0.6185)  loss_giou_2_unscaled: 0.8267 (0.8267)  cardinality_error_2_unscaled: 298.0000 (298.0000)  loss_ce_3_unscaled: 1.1764 (1.1764)  loss_bbox_3_unscaled: 0.6240 (0.6240)  loss_giou_3_unscaled: 0.8267 (0.8267)  cardinality_error_3_unscaled: 298.0000 (298.0000)  loss_ce_4_unscaled: 1.2079 (1.2079)  loss_bbox_4_unscaled: 0.6194 (0.6194)  loss_giou_4_unscaled: 0.8267 (0.8267)  cardinality_error_4_unscaled: 298.0000 (298.0000)  time: 8.1187  data: 0.0001  max mem: 3350\n",
      "Epoch: [0]  [   10/59143]  eta: 22:01:33  lr: 0.000200  class_error: 80.00  grad_norm: 46.81  loss: 33.3648 (36.2904)  loss_ce: 2.1785 (2.1963)  loss_bbox: 1.5498 (2.2176)  loss_giou: 1.7153 (1.7133)  loss_ce_0: 1.9075 (1.9229)  loss_bbox_0: 1.5887 (2.2513)  loss_giou_0: 1.7174 (1.7201)  loss_ce_1: 2.0152 (2.0892)  loss_bbox_1: 1.5729 (2.2313)  loss_giou_1: 1.7112 (1.7123)  loss_ce_2: 2.0890 (2.1325)  loss_bbox_2: 1.5528 (2.2229)  loss_giou_2: 1.6921 (1.7090)  loss_ce_3: 2.0952 (2.1600)  loss_bbox_3: 1.5378 (2.2211)  loss_giou_3: 1.7062 (1.7098)  loss_ce_4: 2.0790 (2.1679)  loss_bbox_4: 1.5472 (2.2047)  loss_giou_4: 1.7151 (1.7082)  loss_ce_unscaled: 1.0892 (1.0981)  class_error_unscaled: 95.6522 (86.0820)  loss_bbox_unscaled: 0.3100 (0.4435)  loss_giou_unscaled: 0.8577 (0.8566)  cardinality_error_unscaled: 292.0000 (292.7273)  loss_ce_0_unscaled: 0.9538 (0.9615)  loss_bbox_0_unscaled: 0.3177 (0.4503)  loss_giou_0_unscaled: 0.8587 (0.8601)  cardinality_error_0_unscaled: 292.0000 (292.2273)  loss_ce_1_unscaled: 1.0076 (1.0446)  loss_bbox_1_unscaled: 0.3146 (0.4463)  loss_giou_1_unscaled: 0.8556 (0.8562)  cardinality_error_1_unscaled: 292.0000 (292.7273)  loss_ce_2_unscaled: 1.0445 (1.0663)  loss_bbox_2_unscaled: 0.3106 (0.4446)  loss_giou_2_unscaled: 0.8461 (0.8545)  cardinality_error_2_unscaled: 292.0000 (292.7273)  loss_ce_3_unscaled: 1.0476 (1.0800)  loss_bbox_3_unscaled: 0.3076 (0.4442)  loss_giou_3_unscaled: 0.8531 (0.8549)  cardinality_error_3_unscaled: 292.0000 (292.7273)  loss_ce_4_unscaled: 1.0395 (1.0840)  loss_bbox_4_unscaled: 0.3094 (0.4409)  loss_giou_4_unscaled: 0.8576 (0.8541)  cardinality_error_4_unscaled: 292.0000 (292.7273)  time: 1.3409  data: 0.0000  max mem: 7263\n",
      "Epoch: [0]  [   20/59143]  eta: 16:35:11  lr: 0.000200  class_error: 100.00  grad_norm: 473.13  loss: 33.3648 (37.7681)  loss_ce: 2.0426 (2.3451)  loss_bbox: 1.5498 (2.3090)  loss_giou: 1.7665 (1.7363)  loss_ce_0: 1.9024 (2.0503)  loss_bbox_0: 1.5887 (2.3384)  loss_giou_0: 1.7779 (1.7455)  loss_ce_1: 2.0018 (2.2827)  loss_bbox_1: 1.5729 (2.3097)  loss_giou_1: 1.7684 (1.7365)  loss_ce_2: 2.0688 (2.1725)  loss_bbox_2: 1.5528 (2.3072)  loss_giou_2: 1.7696 (1.7350)  loss_ce_3: 2.0649 (2.2738)  loss_bbox_3: 1.5378 (2.3076)  loss_giou_3: 1.7683 (1.7344)  loss_ce_4: 2.0630 (2.3485)  loss_bbox_4: 1.5472 (2.3012)  loss_giou_4: 1.7678 (1.7344)  loss_ce_unscaled: 1.0213 (1.1725)  class_error_unscaled: 100.0000 (84.1634)  loss_bbox_unscaled: 0.3100 (0.4618)  loss_giou_unscaled: 0.8832 (0.8681)  cardinality_error_unscaled: 292.5000 (292.8810)  loss_ce_0_unscaled: 0.9512 (1.0252)  loss_bbox_0_unscaled: 0.3177 (0.4677)  loss_giou_0_unscaled: 0.8889 (0.8728)  cardinality_error_0_unscaled: 292.5000 (292.6190)  loss_ce_1_unscaled: 1.0009 (1.1414)  loss_bbox_1_unscaled: 0.3146 (0.4619)  loss_giou_1_unscaled: 0.8842 (0.8682)  cardinality_error_1_unscaled: 292.5000 (292.8810)  loss_ce_2_unscaled: 1.0344 (1.0863)  loss_bbox_2_unscaled: 0.3106 (0.4614)  loss_giou_2_unscaled: 0.8848 (0.8675)  cardinality_error_2_unscaled: 292.5000 (292.8810)  loss_ce_3_unscaled: 1.0324 (1.1369)  loss_bbox_3_unscaled: 0.3076 (0.4615)  loss_giou_3_unscaled: 0.8841 (0.8672)  cardinality_error_3_unscaled: 292.5000 (292.8810)  loss_ce_4_unscaled: 1.0315 (1.1743)  loss_bbox_4_unscaled: 0.3094 (0.4602)  loss_giou_4_unscaled: 0.8839 (0.8672)  cardinality_error_4_unscaled: 292.5000 (292.8810)  time: 0.6545  data: 0.0000  max mem: 7263\n",
      "Epoch: [0]  [   30/59143]  eta: 14:25:13  lr: 0.000200  class_error: 79.31  grad_norm: 42.77  loss: 32.0464 (35.6781)  loss_ce: 1.9815 (2.2168)  loss_bbox: 1.6223 (2.1039)  loss_giou: 1.7496 (1.7138)  loss_ce_0: 1.8055 (1.9468)  loss_bbox_0: 1.6520 (2.1359)  loss_giou_0: 1.7536 (1.7231)  loss_ce_1: 1.8319 (2.1531)  loss_bbox_1: 1.6560 (2.1078)  loss_giou_1: 1.7453 (1.7143)  loss_ce_2: 1.8944 (2.0842)  loss_bbox_2: 1.6331 (2.1026)  loss_giou_2: 1.7467 (1.7120)  loss_ce_3: 1.8864 (2.1397)  loss_bbox_3: 1.6396 (2.1035)  loss_giou_3: 1.7480 (1.7124)  loss_ce_4: 1.8543 (2.1965)  loss_bbox_4: 1.6391 (2.0992)  loss_giou_4: 1.7449 (1.7124)  loss_ce_unscaled: 0.9908 (1.1084)  class_error_unscaled: 85.7143 (81.3686)  loss_bbox_unscaled: 0.3245 (0.4208)  loss_giou_unscaled: 0.8748 (0.8569)  cardinality_error_unscaled: 292.5000 (292.6290)  loss_ce_0_unscaled: 0.9028 (0.9734)  loss_bbox_0_unscaled: 0.3304 (0.4272)  loss_giou_0_unscaled: 0.8768 (0.8615)  cardinality_error_0_unscaled: 292.5000 (292.4516)  loss_ce_1_unscaled: 0.9159 (1.0766)  loss_bbox_1_unscaled: 0.3312 (0.4216)  loss_giou_1_unscaled: 0.8727 (0.8571)  cardinality_error_1_unscaled: 292.5000 (292.6290)  loss_ce_2_unscaled: 0.9472 (1.0421)  loss_bbox_2_unscaled: 0.3266 (0.4205)  loss_giou_2_unscaled: 0.8733 (0.8560)  cardinality_error_2_unscaled: 292.5000 (292.6290)  loss_ce_3_unscaled: 0.9432 (1.0699)  loss_bbox_3_unscaled: 0.3279 (0.4207)  loss_giou_3_unscaled: 0.8740 (0.8562)  cardinality_error_3_unscaled: 292.5000 (292.6290)  loss_ce_4_unscaled: 0.9271 (1.0983)  loss_bbox_4_unscaled: 0.3278 (0.4198)  loss_giou_4_unscaled: 0.8725 (0.8562)  cardinality_error_4_unscaled: 292.5000 (292.6290)  time: 0.6237  data: 0.0000  max mem: 7263\n",
      "Epoch: [0]  [   40/59143]  eta: 14:09:08  lr: 0.000200  class_error: 100.00  grad_norm: 67.09  loss: 31.0054 (35.3729)  loss_ce: 2.0317 (2.1625)  loss_bbox: 1.6047 (2.0778)  loss_giou: 1.7330 (1.7202)  loss_ce_0: 1.8057 (1.9399)  loss_bbox_0: 1.6425 (2.1085)  loss_giou_0: 1.7466 (1.7313)  loss_ce_1: 1.8945 (2.1086)  loss_bbox_1: 1.6188 (2.0821)  loss_giou_1: 1.7329 (1.7203)  loss_ce_2: 1.9568 (2.0739)  loss_bbox_2: 1.6073 (2.0776)  loss_giou_2: 1.7332 (1.7177)  loss_ce_3: 1.9774 (2.1116)  loss_bbox_3: 1.5974 (2.0764)  loss_giou_3: 1.7341 (1.7175)  loss_ce_4: 2.0218 (2.1542)  loss_bbox_4: 1.5935 (2.0737)  loss_giou_4: 1.7325 (1.7192)  loss_ce_unscaled: 1.0158 (1.0812)  class_error_unscaled: 82.3529 (80.9623)  loss_bbox_unscaled: 0.3209 (0.4156)  loss_giou_unscaled: 0.8665 (0.8601)  cardinality_error_unscaled: 292.5000 (292.7927)  loss_ce_0_unscaled: 0.9029 (0.9700)  loss_bbox_0_unscaled: 0.3285 (0.4217)  loss_giou_0_unscaled: 0.8733 (0.8656)  cardinality_error_0_unscaled: 292.5000 (292.6585)  loss_ce_1_unscaled: 0.9473 (1.0543)  loss_bbox_1_unscaled: 0.3238 (0.4164)  loss_giou_1_unscaled: 0.8664 (0.8602)  cardinality_error_1_unscaled: 292.5000 (292.7927)  loss_ce_2_unscaled: 0.9784 (1.0369)  loss_bbox_2_unscaled: 0.3215 (0.4155)  loss_giou_2_unscaled: 0.8666 (0.8589)  cardinality_error_2_unscaled: 292.5000 (292.7927)  loss_ce_3_unscaled: 0.9887 (1.0558)  loss_bbox_3_unscaled: 0.3195 (0.4153)  loss_giou_3_unscaled: 0.8671 (0.8587)  cardinality_error_3_unscaled: 292.5000 (292.7927)  loss_ce_4_unscaled: 1.0109 (1.0771)  loss_bbox_4_unscaled: 0.3187 (0.4147)  loss_giou_4_unscaled: 0.8663 (0.8596)  cardinality_error_4_unscaled: 292.5000 (292.7927)  time: 0.7067  data: 0.0000  max mem: 8531\n",
      "Epoch: [0]  [   50/59143]  eta: 13:39:44  lr: 0.000200  class_error: 45.83  grad_norm: 53.03  loss: 30.7842 (35.4827)  loss_ce: 1.8093 (2.1070)  loss_bbox: 1.6390 (2.1591)  loss_giou: 1.6887 (1.7006)  loss_ce_0: 1.7662 (1.9105)  loss_bbox_0: 1.6507 (2.1942)  loss_giou_0: 1.6997 (1.7136)  loss_ce_1: 1.8622 (2.0689)  loss_bbox_1: 1.6440 (2.1586)  loss_giou_1: 1.6781 (1.7000)  loss_ce_2: 1.8442 (2.0340)  loss_bbox_2: 1.6485 (2.1574)  loss_giou_2: 1.6809 (1.6977)  loss_ce_3: 1.8443 (2.0693)  loss_bbox_3: 1.6336 (2.1562)  loss_giou_3: 1.6805 (1.6972)  loss_ce_4: 1.8334 (2.1063)  loss_bbox_4: 1.6316 (2.1534)  loss_giou_4: 1.6786 (1.6987)  loss_ce_unscaled: 0.9047 (1.0535)  class_error_unscaled: 80.0000 (79.3570)  loss_bbox_unscaled: 0.3278 (0.4318)  loss_giou_unscaled: 0.8444 (0.8503)  cardinality_error_unscaled: 293.0000 (292.8235)  loss_ce_0_unscaled: 0.8831 (0.9553)  loss_bbox_0_unscaled: 0.3301 (0.4388)  loss_giou_0_unscaled: 0.8499 (0.8568)  cardinality_error_0_unscaled: 293.0000 (292.7157)  loss_ce_1_unscaled: 0.9311 (1.0344)  loss_bbox_1_unscaled: 0.3288 (0.4317)  loss_giou_1_unscaled: 0.8391 (0.8500)  cardinality_error_1_unscaled: 293.0000 (292.8235)  loss_ce_2_unscaled: 0.9221 (1.0170)  loss_bbox_2_unscaled: 0.3297 (0.4315)  loss_giou_2_unscaled: 0.8404 (0.8488)  cardinality_error_2_unscaled: 293.0000 (292.8235)  loss_ce_3_unscaled: 0.9221 (1.0346)  loss_bbox_3_unscaled: 0.3267 (0.4312)  loss_giou_3_unscaled: 0.8402 (0.8486)  cardinality_error_3_unscaled: 293.0000 (292.8235)  loss_ce_4_unscaled: 0.9167 (1.0531)  loss_bbox_4_unscaled: 0.3263 (0.4307)  loss_giou_4_unscaled: 0.8393 (0.8493)  cardinality_error_4_unscaled: 293.0000 (292.8235)  time: 0.7612  data: 0.0000  max mem: 8531\n",
      "Epoch: [0]  [   60/59143]  eta: 13:11:44  lr: 0.000200  class_error: 12.50  grad_norm: 114.92  loss: 34.2189 (35.3765)  loss_ce: 1.8978 (2.0806)  loss_bbox: 2.1628 (2.1828)  loss_giou: 1.6217 (1.6734)  loss_ce_0: 1.7694 (1.9059)  loss_bbox_0: 2.0914 (2.2229)  loss_giou_0: 1.6218 (1.6853)  loss_ce_1: 1.8957 (2.0516)  loss_bbox_1: 2.1650 (2.1828)  loss_giou_1: 1.5928 (1.6734)  loss_ce_2: 1.8675 (2.0269)  loss_bbox_2: 2.1656 (2.1830)  loss_giou_2: 1.5944 (1.6717)  loss_ce_3: 1.8624 (2.0546)  loss_bbox_3: 2.1772 (2.1800)  loss_giou_3: 1.5967 (1.6717)  loss_ce_4: 1.8399 (2.0808)  loss_bbox_4: 2.1661 (2.1774)  loss_giou_4: 1.6073 (1.6717)  loss_ce_unscaled: 0.9489 (1.0403)  class_error_unscaled: 80.0000 (78.3194)  loss_bbox_unscaled: 0.4326 (0.4366)  loss_giou_unscaled: 0.8109 (0.8367)  cardinality_error_unscaled: 295.5000 (293.4180)  loss_ce_0_unscaled: 0.8847 (0.9530)  loss_bbox_0_unscaled: 0.4183 (0.4446)  loss_giou_0_unscaled: 0.8109 (0.8427)  cardinality_error_0_unscaled: 295.5000 (293.3279)  loss_ce_1_unscaled: 0.9478 (1.0258)  loss_bbox_1_unscaled: 0.4330 (0.4366)  loss_giou_1_unscaled: 0.7964 (0.8367)  cardinality_error_1_unscaled: 295.5000 (293.4180)  loss_ce_2_unscaled: 0.9338 (1.0134)  loss_bbox_2_unscaled: 0.4331 (0.4366)  loss_giou_2_unscaled: 0.7972 (0.8358)  cardinality_error_2_unscaled: 295.5000 (293.4180)  loss_ce_3_unscaled: 0.9312 (1.0273)  loss_bbox_3_unscaled: 0.4354 (0.4360)  loss_giou_3_unscaled: 0.7984 (0.8359)  cardinality_error_3_unscaled: 295.5000 (293.4180)  loss_ce_4_unscaled: 0.9200 (1.0404)  loss_bbox_4_unscaled: 0.4332 (0.4355)  loss_giou_4_unscaled: 0.8036 (0.8358)  cardinality_error_4_unscaled: 295.5000 (293.4180)  time: 0.6851  data: 0.0000  max mem: 8531\n",
      "Epoch: [0]  [   70/59143]  eta: 12:58:31  lr: 0.000200  class_error: 83.33  grad_norm: 46.91  loss: 33.9439 (35.3282)  loss_ce: 1.9330 (2.0445)  loss_bbox: 2.1628 (2.2083)  loss_giou: 1.5976 (1.6715)  loss_ce_0: 1.7694 (1.8808)  loss_bbox_0: 2.1864 (2.2465)  loss_giou_0: 1.6081 (1.6824)  loss_ce_1: 1.9080 (2.0200)  loss_bbox_1: 2.1908 (2.2118)  loss_giou_1: 1.5928 (1.6714)  loss_ce_2: 1.8996 (1.9950)  loss_bbox_2: 2.1656 (2.2106)  loss_giou_2: 1.5895 (1.6701)  loss_ce_3: 1.9081 (2.0194)  loss_bbox_3: 2.1772 (2.2073)  loss_giou_3: 1.5922 (1.6699)  loss_ce_4: 1.8677 (2.0409)  loss_bbox_4: 2.1661 (2.2068)  loss_giou_4: 1.5892 (1.6707)  loss_ce_unscaled: 0.9665 (1.0222)  class_error_unscaled: 80.0000 (77.7253)  loss_bbox_unscaled: 0.4326 (0.4417)  loss_giou_unscaled: 0.7988 (0.8358)  cardinality_error_unscaled: 295.0000 (293.4507)  loss_ce_0_unscaled: 0.8847 (0.9404)  loss_bbox_0_unscaled: 0.4373 (0.4493)  loss_giou_0_unscaled: 0.8041 (0.8412)  cardinality_error_0_unscaled: 295.0000 (293.3732)  loss_ce_1_unscaled: 0.9540 (1.0100)  loss_bbox_1_unscaled: 0.4382 (0.4424)  loss_giou_1_unscaled: 0.7964 (0.8357)  cardinality_error_1_unscaled: 295.0000 (293.4507)  loss_ce_2_unscaled: 0.9498 (0.9975)  loss_bbox_2_unscaled: 0.4331 (0.4421)  loss_giou_2_unscaled: 0.7948 (0.8351)  cardinality_error_2_unscaled: 295.0000 (293.4507)  loss_ce_3_unscaled: 0.9540 (1.0097)  loss_bbox_3_unscaled: 0.4354 (0.4415)  loss_giou_3_unscaled: 0.7961 (0.8350)  cardinality_error_3_unscaled: 295.0000 (293.4507)  loss_ce_4_unscaled: 0.9339 (1.0205)  loss_bbox_4_unscaled: 0.4332 (0.4414)  loss_giou_4_unscaled: 0.7946 (0.8354)  cardinality_error_4_unscaled: 295.0000 (293.4507)  time: 0.6847  data: 0.0000  max mem: 8531\n",
      "Epoch: [0]  [   80/59143]  eta: 12:36:35  lr: 0.000200  class_error: 64.71  grad_norm: 52.16  loss: 30.8606 (34.8239)  loss_ce: 1.7054 (1.9985)  loss_bbox: 1.8824 (2.1690)  loss_giou: 1.6497 (1.6697)  loss_ce_0: 1.6149 (1.8461)  loss_bbox_0: 1.8750 (2.2040)  loss_giou_0: 1.6774 (1.6804)  loss_ce_1: 1.7325 (1.9767)  loss_bbox_1: 1.9102 (2.1734)  loss_giou_1: 1.6542 (1.6691)  loss_ce_2: 1.7310 (1.9550)  loss_bbox_2: 1.9065 (2.1709)  loss_giou_2: 1.6479 (1.6682)  loss_ce_3: 1.7356 (1.9777)  loss_bbox_3: 1.8847 (2.1662)  loss_giou_3: 1.6542 (1.6682)  loss_ce_4: 1.7443 (1.9954)  loss_bbox_4: 1.8726 (2.1666)  loss_giou_4: 1.6564 (1.6688)  loss_ce_unscaled: 0.8527 (0.9992)  class_error_unscaled: 78.2609 (77.2723)  loss_bbox_unscaled: 0.3765 (0.4338)  loss_giou_unscaled: 0.8248 (0.8349)  cardinality_error_unscaled: 292.0000 (292.9506)  loss_ce_0_unscaled: 0.8074 (0.9230)  loss_bbox_0_unscaled: 0.3750 (0.4408)  loss_giou_0_unscaled: 0.8387 (0.8402)  cardinality_error_0_unscaled: 292.0000 (292.8827)  loss_ce_1_unscaled: 0.8663 (0.9883)  loss_bbox_1_unscaled: 0.3820 (0.4347)  loss_giou_1_unscaled: 0.8271 (0.8345)  cardinality_error_1_unscaled: 292.0000 (292.9506)  loss_ce_2_unscaled: 0.8655 (0.9775)  loss_bbox_2_unscaled: 0.3813 (0.4342)  loss_giou_2_unscaled: 0.8240 (0.8341)  cardinality_error_2_unscaled: 292.0000 (292.9506)  loss_ce_3_unscaled: 0.8678 (0.9889)  loss_bbox_3_unscaled: 0.3769 (0.4332)  loss_giou_3_unscaled: 0.8271 (0.8341)  cardinality_error_3_unscaled: 292.0000 (292.9506)  loss_ce_4_unscaled: 0.8722 (0.9977)  loss_bbox_4_unscaled: 0.3745 (0.4333)  loss_giou_4_unscaled: 0.8282 (0.8344)  cardinality_error_4_unscaled: 292.0000 (292.9506)  time: 0.6606  data: 0.0000  max mem: 8531\n",
      "Epoch: [0]  [   90/59143]  eta: 12:28:35  lr: 0.000200  class_error: 100.00  grad_norm: 75.74  loss: 31.4892 (35.2558)  loss_ce: 1.7728 (2.0050)  loss_bbox: 1.7178 (2.2355)  loss_giou: 1.6478 (1.6668)  loss_ce_0: 1.7256 (1.8548)  loss_bbox_0: 1.7589 (2.2696)  loss_giou_0: 1.6774 (1.6785)  loss_ce_1: 1.7685 (1.9868)  loss_bbox_1: 1.7439 (2.2394)  loss_giou_1: 1.6542 (1.6659)  loss_ce_2: 1.8387 (1.9626)  loss_bbox_2: 1.7342 (2.2385)  loss_giou_2: 1.6479 (1.6659)  loss_ce_3: 1.8660 (1.9853)  loss_bbox_3: 1.7147 (2.2344)  loss_giou_3: 1.6542 (1.6660)  loss_ce_4: 1.7934 (2.0018)  loss_bbox_4: 1.7158 (2.2329)  loss_giou_4: 1.6564 (1.6661)  loss_ce_unscaled: 0.8864 (1.0025)  class_error_unscaled: 80.0000 (77.8384)  loss_bbox_unscaled: 0.3436 (0.4471)  loss_giou_unscaled: 0.8239 (0.8334)  cardinality_error_unscaled: 292.0000 (293.1044)  loss_ce_0_unscaled: 0.8628 (0.9274)  loss_bbox_0_unscaled: 0.3518 (0.4539)  loss_giou_0_unscaled: 0.8387 (0.8393)  cardinality_error_0_unscaled: 292.0000 (293.0440)  loss_ce_1_unscaled: 0.8842 (0.9934)  loss_bbox_1_unscaled: 0.3488 (0.4479)  loss_giou_1_unscaled: 0.8271 (0.8330)  cardinality_error_1_unscaled: 292.0000 (293.1044)  loss_ce_2_unscaled: 0.9194 (0.9813)  loss_bbox_2_unscaled: 0.3468 (0.4477)  loss_giou_2_unscaled: 0.8240 (0.8330)  cardinality_error_2_unscaled: 292.0000 (293.1044)  loss_ce_3_unscaled: 0.9330 (0.9926)  loss_bbox_3_unscaled: 0.3429 (0.4469)  loss_giou_3_unscaled: 0.8271 (0.8330)  cardinality_error_3_unscaled: 292.0000 (293.1044)  loss_ce_4_unscaled: 0.8967 (1.0009)  loss_bbox_4_unscaled: 0.3432 (0.4466)  loss_giou_4_unscaled: 0.8282 (0.8330)  cardinality_error_4_unscaled: 292.0000 (293.1044)  time: 0.6536  data: 0.0000  max mem: 8531\n",
      "Epoch: [0]  [  100/59143]  eta: 12:19:57  lr: 0.000200  class_error: 70.00  grad_norm: 88.57  loss: 33.0423 (35.4335)  loss_ce: 1.9526 (1.9972)  loss_bbox: 2.1200 (2.2807)  loss_giou: 1.5829 (1.6556)  loss_ce_0: 1.8424 (1.8564)  loss_bbox_0: 2.0894 (2.3132)  loss_giou_0: 1.5771 (1.6675)  loss_ce_1: 1.9754 (1.9808)  loss_bbox_1: 2.0825 (2.2824)  loss_giou_1: 1.5697 (1.6547)  loss_ce_2: 1.9743 (1.9604)  loss_bbox_2: 2.1077 (2.2830)  loss_giou_2: 1.5752 (1.6551)  loss_ce_3: 1.9691 (1.9838)  loss_bbox_3: 2.1136 (2.2802)  loss_giou_3: 1.5810 (1.6548)  loss_ce_4: 1.9396 (1.9925)  loss_bbox_4: 2.1126 (2.2796)  loss_giou_4: 1.5835 (1.6556)  loss_ce_unscaled: 0.9763 (0.9986)  class_error_unscaled: 87.5000 (79.0421)  loss_bbox_unscaled: 0.4240 (0.4561)  loss_giou_unscaled: 0.7915 (0.8278)  cardinality_error_unscaled: 296.5000 (293.4208)  loss_ce_0_unscaled: 0.9212 (0.9282)  loss_bbox_0_unscaled: 0.4179 (0.4626)  loss_giou_0_unscaled: 0.7885 (0.8338)  cardinality_error_0_unscaled: 296.5000 (293.3663)  loss_ce_1_unscaled: 0.9877 (0.9904)  loss_bbox_1_unscaled: 0.4165 (0.4565)  loss_giou_1_unscaled: 0.7848 (0.8273)  cardinality_error_1_unscaled: 296.5000 (293.4208)  loss_ce_2_unscaled: 0.9871 (0.9802)  loss_bbox_2_unscaled: 0.4215 (0.4566)  loss_giou_2_unscaled: 0.7876 (0.8275)  cardinality_error_2_unscaled: 296.5000 (293.4208)  loss_ce_3_unscaled: 0.9845 (0.9919)  loss_bbox_3_unscaled: 0.4227 (0.4560)  loss_giou_3_unscaled: 0.7905 (0.8274)  cardinality_error_3_unscaled: 296.5000 (293.4208)  loss_ce_4_unscaled: 0.9698 (0.9962)  loss_bbox_4_unscaled: 0.4225 (0.4559)  loss_giou_4_unscaled: 0.7918 (0.8278)  cardinality_error_4_unscaled: 296.5000 (293.4208)  time: 0.6845  data: 0.0000  max mem: 8531\n"
     ]
    }
   ]
  }
 ]
}
